{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d304754-7df8-4982-a6c2-b948e89f0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Country  Energy_Consumption  Industrial_Production  Transportation  \\\n",
      "0  2029      USA                5000                    120             195   \n",
      "1  2030    India                4000                    141             250   \n",
      "2  2020      USA                4000                    110             150   \n",
      "3  2027  Germany                3200                    128             135   \n",
      "4  2020   Brazil                1500                    105              80   \n",
      "\n",
      "   Weather_Patterns  Carbon_Emissions  \n",
      "0                96              6100  \n",
      "1                98              7500  \n",
      "2               105              5200  \n",
      "3               100              4200  \n",
      "4               102              2200  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66 entries, 0 to 65\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Year                   66 non-null     int64 \n",
      " 1   Country                66 non-null     object\n",
      " 2   Energy_Consumption     66 non-null     int64 \n",
      " 3   Industrial_Production  66 non-null     int64 \n",
      " 4   Transportation         66 non-null     int64 \n",
      " 5   Weather_Patterns       66 non-null     int64 \n",
      " 6   Carbon_Emissions       66 non-null     int64 \n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 3.7+ KB\n",
      "None\n",
      "Year                     0\n",
      "Country                  0\n",
      "Energy_Consumption       0\n",
      "Industrial_Production    0\n",
      "Transportation           0\n",
      "Weather_Patterns         0\n",
      "Carbon_Emissions         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"carbon_emissions_small.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check dataset info\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a130e594-d670-40c3-aea3-f324b4fb97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Normalize numeric features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699b9f6e-eff1-4b84-9e5b-2f4914250afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define target (assume 'Emissions' is the target variable)\n",
    "X = df.drop(columns=['Carbon_Emissions'])\n",
    "y = df['Carbon_Emissions']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9beb3d2b-e911-4f8f-a0e1-96402f655190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Energy_Consumption', 'Industrial_Production', 'Transportation',\n",
      "       'Weather_Patterns', 'Carbon_Emissions', 'Country_China',\n",
      "       'Country_Germany', 'Country_India', 'Country_Japan', 'Country_USA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14558631-89f3-41c7-8169-e0b0abc1225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00016581548793087337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92255aa5-db6b-43a6-acd6-7c0df8fff64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('carbon_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d421b7cd-2e9d-42c2-bcd0-c11a2cc21c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (52, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c612ea7c-64a8-4090-833c-56e92db98bdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[43minput_data\u001b[49m)  \u001b[38;5;66;03m# If you used StandardScaler\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "input_data = scaler.transform(input_data)  # If you used StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2d26c5-95fe-407d-86ec-63f77df7a61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')  # Save scaler after fitting on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01393df4-becc-4c9c-87cc-59f4a59896d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X_train is your training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit the scaler on training data\n",
    "\n",
    "# Save the trained scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')  # This saves the scaler to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e298d9f-0a3f-4df2-89a8-6dc17482ca0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_names.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"carbon_emissions.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Carbon_Emissions\"])  # Features\n",
    "y = df[\"Carbon_Emissions\"]  # Target\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)  # Example: Converts 'Country' to 'Country_China', 'Country_USA', etc.\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler and feature names\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(X.columns, \"feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "107a9934-d61c-4ecc-9adc-1f1f9c2c5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model, scaler, and feature names saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset (Replace 'your_data.csv' with your actual dataset)\n",
    "df = pd.read_csv(\"carbon_emissions_small.csv\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"Carbon_Emissions\"])  # Replace with actual target column name\n",
    "y = df[\"Carbon_Emissions\"]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained model, scaler, and feature names\n",
    "joblib.dump(model, \"model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(X.columns.tolist(), \"feature_names.pkl\")\n",
    "\n",
    "print(\"âœ… Model, scaler, and feature names saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186380dc-cf64-4dce-9924-b4aaed2177cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
